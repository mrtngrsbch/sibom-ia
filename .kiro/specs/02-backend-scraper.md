# Arquitectura del Backend: Python CLI Scraper

## Introducci√≥n

El backend del SIBOM Scraper es una herramienta CLI en Python que implementa un sistema de extracci√≥n de datos de 3 niveles con procesamiento h√≠brido. Ubicado en `python-cli/`, representa la parte de "extracci√≥n" del ecosistema.

## Arquitectura Principal

### Clase Central: SIBOMScraper

**Ubicaci√≥n**: `python-cli/sibom_scraper.py:32-848`

```python
class SIBOMScraper:
    def __init__(self, api_key: str, model: str = "z-ai/glm-4.5-air:free"):
        self.client = OpenAI(
            api_key=api_key,
            base_url="https://openrouter.ai/api/v1"
        )
        self.model = model
        self.rate_limit_delay = 3  # segundos entre llamadas
        self.last_call_time = 0
```

**Patr√≥n de Dise√±o**: Facade + Strategy Pattern
- **Facade**: Simplifica la complejidad del scraping en una interfaz √∫nica
- **Strategy**: Permite cambiar modelos LLM din√°micamente

### Pipeline de 3 Niveles

#### Nivel 1: Extracci√≥n de Listados
**M√©todo**: `parse_listing_page(html: str, url: str) -> List[Dict]`
**L√≠neas**: `python-cli/sibom_scraper.py:200-250`

```python
def parse_listing_page(self, html: str, url: str) -> List[Dict]:
    """Nivel 1: Extrae listado de boletines usando BeautifulSoup (con fallback a LLM)"""
    try:
        # Intentar con BeautifulSoup primero (95% de casos)
        soup = BeautifulSoup(html, 'lxml')
        bulletin_divs = soup.find_all('div', class_='row bulletin')
        
        for bulletin_div in bulletin_divs:
            title_elem = bulletin_div.find('p', class_='bulletin-title')
            date_elem = bulletin_div.find('p', class_='bulletin-date')
            form_elem = bulletin_div.find('form', class_='button_to')
            # ... extracci√≥n de metadatos
            
    except Exception as e:
        # Fallback a LLM si BeautifulSoup falla
        prompt = f"""Analiza este HTML de SIBOM..."""
        response = self._make_llm_call(prompt, use_json_mode=True)
```

**Estrategia H√≠brida**:
- **Primario**: BeautifulSoup (r√°pido, gratis, 95% casos)
- **Fallback**: LLM (robusto, costoso, 5% casos complejos)

#### Nivel 2: Extracci√≥n de Enlaces
**M√©todo**: `parse_bulletin_content_links(html: str) -> List[str]`
**L√≠neas**: `python-cli/sibom_scraper.py:320-380`

```python
def parse_bulletin_content_links(self, html: str) -> List[str]:
    """Nivel 2: Extrae enlaces de contenido espec√≠fico"""
    try:
        soup = BeautifulSoup(html, 'lxml')
        content_links = soup.find_all('a', class_='content-link')
        links = [link.get('href', '') for link in content_links if link.get('href')]
        
        if links:
            return links
        else:
            raise ValueError("No se encontraron enlaces con BeautifulSoup")
    except Exception as e:
        # Fallback a LLM con manejo robusto de JSON malformado
        response = self._make_llm_call(prompt, use_json_mode=True)
        # ... manejo de errores JSON con extracci√≥n manual
```

**Manejo de Errores JSON**: `python-cli/sibom_scraper.py:360-378`
- Intenta parsear JSON normal
- Si falla, extrae manualmente el primer objeto JSON v√°lido
- Logging detallado para debugging

#### Nivel 3: Extracci√≥n de Contenido
**M√©todo**: `parse_final_content(html: str) -> str`
**L√≠neas**: `python-cli/sibom_scraper.py:380-450`

```python
def parse_final_content(self, html: str) -> str:
    """Nivel 3: Extrae texto completo usando BeautifulSoup mejorado (sin LLM)"""
    
    # Estrategia 1: Buscar contenedor principal por ID
    container = soup.find('div', id='frontend-container')
    
    if not container:
        # Estrategia 2: Buscar por clase que contenga 'content'
        container = soup.find('div', class_=lambda x: x and 'content' in str(x).lower())
    
    if not container:
        # Estrategia 3: Elementos sem√°nticos
        container = soup.find('main') or soup.find('article')
    
    if not container:
        # Estrategia 4: Body limpio
        body = soup.find('body')
        for unwanted in body.find_all(['script', 'style', 'nav', 'footer']):
            unwanted.decompose()
        container = body
```

**Optimizaci√≥n**: Solo BeautifulSoup (sin LLM) para reducir costos

## Caracter√≠sticas Avanzadas

### Detecci√≥n Autom√°tica de Paginaci√≥n
**M√©todo**: `detect_total_pages(html: str) -> int`
**L√≠neas**: `python-cli/sibom_scraper.py:252-318`

```python
def detect_total_pages(self, html: str) -> int:
    """Detecta n√∫mero total de p√°ginas usando BeautifulSoup"""
    soup = BeautifulSoup(html, 'lxml')
    pagination = soup.find('ul', class_='pagination')
    
    # Buscar enlace "√öltima" que contiene el n√∫mero total
    last_page_link = pagination.find('a', string=lambda text: text and '√öltima' in text)
    
    if last_page_link:
        href = last_page_link.get('href', '')
        match = re.search(r'page=(\d+)', href)
        if match:
            return int(match.group(1))
```

**Ventajas**:
- ‚úÖ **Zero intervenci√≥n manual**: No m√°s copy-paste de URLs
- ‚úÖ **Costo $0**: Detecci√≥n con BeautifulSoup, sin LLM
- ‚úÖ **R√°pido**: 14 p√°ginas en ~2 segundos

### Procesamiento Paralelo
**M√©todo**: `scrape()` con ThreadPoolExecutor
**L√≠neas**: `python-cli/sibom_scraper.py:700-730`

```python
if parallel > 1:
    with ThreadPoolExecutor(max_workers=parallel) as executor:
        futures = {executor.submit(self.process_bulletin, b, base_url, output_dir, skip_existing): b 
                  for b in bulletins}
        
        with Progress(SpinnerColumn(), TextColumn(), BarColumn()) as progress:
            task = progress.add_task(f"[cyan]Procesando...", total=len(bulletins))
            
            for future in as_completed(futures):
                result = future.result()
                results.append(result)
                progress.update(task, advance=1)
```

**Configuraci√≥n**:
- **Default**: 1 hilo (secuencial)
- **Recomendado**: 3 hilos (`--parallel 3`)
- **Performance**: 2-3s por bolet√≠n vs. 5-7s secuencial

### Rate Limiting Inteligente
**M√©todo**: `_wait_for_rate_limit()`
**L√≠neas**: `python-cli/sibom_scraper.py:42-48`

```python
def _wait_for_rate_limit(self):
    """Espera seg√∫n rate limiting"""
    elapsed = time.time() - self.last_call_time
    if elapsed < self.rate_limit_delay:
        time.sleep(self.rate_limit_delay - elapsed)
    self.last_call_time = time.time()
```

**Configuraci√≥n**: 3 segundos entre llamadas LLM (configurable)

### Gesti√≥n de Archivos Existentes
**M√©todo**: `process_bulletin()` con men√∫ interactivo
**L√≠neas**: `python-cli/sibom_scraper.py:520-580`

```python
if filepath.exists():
    if skip_existing:
        # Modo autom√°tico: saltar sin preguntar
        return existing_data
    else:
        # Modo interactivo: men√∫ de opciones
        console.print("¬øQu√© deseas hacer con el bolet√≠n?")
        console.print("  [cyan]1.[/cyan] Saltar y continuar")
        console.print("  [cyan]2.[/cyan] Sobreescribir")
        console.print("  [cyan]3.[/cyan] Cancelar todo")
        
        choice = input("\nElige una opci√≥n (1-3) [1]: ").strip() or "1"
```

**Modos**:
- **Interactivo** (default): Pregunta al usuario
- **Autom√°tico** (`--skip-existing`): Salta autom√°ticamente

## Sanitizaci√≥n de Nombres de Archivo

**M√©todo**: `_sanitize_filename(description: str, number: str) -> str`
**L√≠neas**: `python-cli/sibom_scraper.py:60-100`

```python
def _sanitize_filename(self, description: str, number: str = None) -> str:
    """
    Convierte descripci√≥n en nombre v√°lido.
    Ejemplo: "105¬∫ de Carlos Tejedor" -> "Carlos_Tejedor_105"
    """
    # Extraer n√∫mero del bolet√≠n
    number_match = re.search(r'(\d+)', number or description)
    num = number_match.group(1) if number_match else "0"
    
    # Para descripciones largas, extraer nombre de ciudad
    if len(description) > 50:
        city_match = re.search(r'(?:de\s+)([A-Z][a-zA-Z\s]+)', description)
        if city_match:
            cleaned = city_match.group(1).strip()
            return f"{cleaned}_{num}"
    
    # Limpiar caracteres especiales
    cleaned = re.sub(r'[^\w\s-]', '', description)
    cleaned = re.sub(r'\s+', '_', cleaned.strip())
    
    return f"{cleaned}_{num}"
```

**Patrones**:
- Descripciones cortas: `"105¬∫ de Carlos Tejedor"` ‚Üí `"Carlos_Tejedor_105"`
- Descripciones largas: `"Bolet√≠n Municipal de Carlos Tejedor..."` ‚Üí `"Carlos_Tejedor_98"`

## Sistema de √çndices

### √çndice Markdown Autom√°tico
**M√©todo**: `_update_index_md(bulletin: Dict, output_dir: Path, base_url: str)`
**L√≠neas**: `python-cli/sibom_scraper.py:102-150`

```python
def _update_index_md(self, bulletin: Dict, output_dir: Path, base_url: str):
    """Actualiza boletines.md con informaci√≥n del bolet√≠n procesado"""
    index_file = output_dir / "boletines.md"
    
    # Crear archivo si no existe
    if not index_file.exists():
        with index_file.open('w', encoding='utf-8') as f:
            f.write("# Boletines Procesados\n\n")
            f.write("| Number | Date | Description | Link | Status |\n")
            f.write("|--------|------|-------------|------|--------|\n")
    
    # Status con emojis
    status_display = {
        'completed': '‚úÖ Completado',
        'skipped': 'ü§ñ Creado', 
        'error': '‚ùå Error',
        'no_content': '‚ö†Ô∏è Sin contenido'
    }.get(status, status)
```

**Caracter√≠sticas**:
- üìã Tabla markdown con todos los boletines
- üîó URLs clickeables a SIBOM
- ‚úÖ Status visual con emojis
- üîÑ Actualizaci√≥n autom√°tica
- üìù Compatible con GitHub/GitLab

### Utilidades de Indexaci√≥n

#### `indexar_boletines.py`
**Funci√≥n**: Genera √≠ndice JSON estructurado
```python
def indexar():
    boletines_path = Path('boletines')
    index = []
    
    for file_path in boletines_path.glob('*.json'):
        with open(file_path, 'r', encoding='utf-8') as f:
            data = json.load(f)
        
        entry = {
            'id': file_path.name.replace('.json', ''),
            'municipality': extract_municipality(file_path.name),
            'type': detect_type(full_text),
            'number': data.get('number', '0'),
            'title': data.get('description'),
            'date': data.get('date', ''),
            'url': data.get('link'),
            'status': detect_status(full_text),
            'filename': file_path.name
        }
        index.append(entry)
```

#### `enrich_index_with_types.py`
**Funci√≥n**: Enriquece √≠ndice con tipos de documentos
```python
def extract_document_types(full_text: str) -> Set[str]:
    """Extrae tipos de documentos del texto"""
    patterns = {
        'ordenanza': r'\bOrdenanza\s*N[¬∞¬∫]\s*\d+',
        'decreto': r'\bDecreto\s*N[¬∞¬∫]\s*\d+',
        'resolucion': r'\bResoluci√≥n\s*N[¬∞¬∫]\s*\d+',
        'disposicion': r'\bDisposici√≥n\s*N[¬∞¬∫]\s*\d+',
        'convenio': r'\bConvenio\s*N[¬∞¬∫]\s*\d+',
        'licitacion': r'\bLicitaci√≥n\s*N[¬∞¬∫]\s*\d+',
    }
    
    types_found = set()
    for doc_type, pattern in patterns.items():
        if re.search(pattern, full_text, re.IGNORECASE):
            types_found.add(doc_type)
    
    return types_found
```

#### `comprimir_boletines.py`
**Funci√≥n**: Compresi√≥n gzip para distribuci√≥n
```python
def comprimir_archivo(archivo: Path, mantener_original: bool = False):
    """Comprime archivo JSON con gzip"""
    with open(archivo, 'r', encoding='utf-8') as f:
        contenido = f.read()
    
    archivo_gz = archivo.with_suffix('.json.gz')
    with gzip.open(archivo_gz, 'wt', encoding='utf-8', compresslevel=9) as f:
        f.write(contenido)
    
    # Ahorro: ~533 MB ‚Üí ~100 MB (80% reducci√≥n)
```

## Configuraci√≥n de Modelos LLM

### Modelos Soportados
**Archivo**: `python-cli/MODELOS.md`

| Modelo | Costo | Calidad | Uso Recomendado |
|--------|-------|---------|-----------------|
| `z-ai/glm-4.5-air:free` | **GRATIS** | Buena | Pruebas, experimentaci√≥n |
| `google/gemini-2.5-flash-lite` | Muy bajo | Muy buena | Producci√≥n econ√≥mica |
| `google/gemini-3-flash-preview` | Bajo | Excelente | Balance calidad-precio |
| `x-ai/grok-4.1-fast` | Alto | Premium | M√°xima calidad |

### Configuraci√≥n Din√°mica
```bash
# Modelo gratuito
python3 sibom_scraper.py --model z-ai/glm-4.5-air:free

# Modelo econ√≥mico  
python3 sibom_scraper.py --model google/gemini-2.5-flash-lite

# Modelo premium
python3 sibom_scraper.py --model x-ai/grok-4.1-fast
```

## CLI y Argumentos

### Argumentos Principales
**Archivo**: `python-cli/sibom_scraper.py:750-800`

```python
parser.add_argument('--url', 
    default='https://sibom.slyt.gba.gob.ar/cities/22',
    help='URL de la p√°gina de listado O bolet√≠n individual')

parser.add_argument('--limit', type=int, default=None,
    help='N√∫mero m√°ximo de boletines a procesar')

parser.add_argument('--parallel', type=int, default=1,
    help='N√∫mero de boletines en paralelo')

parser.add_argument('--model', type=str, 
    default='google/gemini-3-flash-preview',
    help='Modelo LLM de OpenRouter')

parser.add_argument('--skip-existing', action='store_true',
    help='Saltar autom√°ticamente boletines existentes')
```

### Ejemplos de Uso
```bash
# Procesar bolet√≠n espec√≠fico
python3 sibom_scraper.py --url https://sibom.slyt.gba.gob.ar/bulletins/13556

# Procesamiento masivo paralelo
python3 sibom_scraper.py --parallel 3 --skip-existing

# P√°gina espec√≠fica con l√≠mite
python3 sibom_scraper.py --url https://sibom.slyt.gba.gob.ar/cities/22?page=6 --limit 5

# Modelo gratuito para pruebas
python3 sibom_scraper.py --limit 5 --model z-ai/glm-4.5-air:free
```

## Estructura de Salida

### Archivos Individuales
**Ubicaci√≥n**: `boletines/{Ciudad}_{Numero}.json`
```json
{
  "number": "105¬∫",
  "date": "30/12/2025", 
  "description": "105¬∫ de Carlos Tejedor",
  "link": "/bulletins/12345",
  "status": "completed",
  "fullText": "[DOC 1]\nORDENANZA N¬∞ 123...\n[DOC 2]\nDECRETO N¬∞ 456..."
}
```

### Resumen Consolidado
**Ubicaci√≥n**: `sibom_results.json` (configurable con `--output`)
```json
[
  {
    "number": "105¬∫",
    "date": "30/12/2025",
    "description": "105¬∫ de Carlos Tejedor", 
    "link": "/bulletins/12345",
    "status": "completed",
    "fullText": "..."
  }
]
```

### √çndice Navegable
**Ubicaci√≥n**: `boletines/boletines.md`
```markdown
| Number | Date | Description | Link | Status |
|--------|------|-------------|------|--------|
| 105¬∫ | 23/12/2025 | 105¬∫ de Carlos Tejedor | [Link](https://sibom.slyt.gba.gob.ar/bulletins/14046) | ‚úÖ Completado |
```

## M√©tricas de Performance

### Velocidad de Procesamiento
- **Secuencial**: 5-7s por bolet√≠n
- **Paralelo x3**: 2-3s por bolet√≠n efectivo
- **100 boletines**: ~3-5 minutos (paralelo) vs. ~10 minutos (secuencial)

### Precisi√≥n de Extracci√≥n
- **BeautifulSoup**: 95% casos exitosos
- **LLM Fallback**: 5% casos complejos
- **Tasa de √©xito global**: >99%

### Costos por Modelo
- **Gratuito**: $0 (z-ai/glm-4.5-air:free)
- **Econ√≥mico**: $0.06 por bolet√≠n (gemini-2.5-flash-lite)
- **Premium**: $0.24 por bolet√≠n (gemini-3-flash-preview)
- **Ultra**: $0.64 por bolet√≠n (grok-4.1-fast)

## Ventajas Arquitecturales

### vs. Versi√≥n React Original
- ‚úÖ **M√°s r√°pido**: Sin proxies CORS, acceso directo
- ‚úÖ **Procesamiento paralelo**: M√∫ltiples boletines simult√°neos  
- ‚úÖ **Menos rate limiting**: Sin restricciones del navegador
- ‚úÖ **M√°s confiable**: Sin problemas de CORS
- ‚úÖ **Port√°til**: Ejecuta en cualquier sistema

### Patrones de Dise√±o Aplicados
- **Hybrid Processing**: BeautifulSoup + LLM fallback
- **Strategy Pattern**: Modelos LLM intercambiables
- **Template Method**: Pipeline de 3 niveles consistente
- **Observer Pattern**: Progress tracking con Rich
- **Factory Pattern**: Creaci√≥n de clientes OpenAI

## Conclusi√≥n

El backend Python representa una soluci√≥n robusta y escalable para web scraping inteligente. Su arquitectura h√≠brida optimiza costos y performance, mientras que las caracter√≠sticas avanzadas como paginaci√≥n autom√°tica y procesamiento paralelo lo hacen adecuado tanto para uso experimental como producci√≥n masiva.