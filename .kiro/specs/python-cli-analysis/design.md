# An√°lisis T√©cnico Completo: Python CLI SIBOM Scraper

## Overview

El proyecto SIBOM Scraper es una herramienta CLI en Python dise√±ada para extraer y procesar boletines oficiales municipales desde el sistema SIBOM. El proyecto demuestra una arquitectura bien estructurada con separaci√≥n clara de responsabilidades, uso inteligente de tecnolog√≠as modernas, y un enfoque pragm√°tico hacia la resoluci√≥n de problemas complejos de web scraping.

## Arquitectura y Componentes

### Arquitectura General

El sistema sigue una **arquitectura de pipeline de 3 niveles** con procesamiento h√≠brido (BeautifulSoup + LLM):

```
Nivel 1: Listado     ‚Üí Nivel 2: Enlaces     ‚Üí Nivel 3: Contenido
(BeautifulSoup)        (BeautifulSoup+LLM)    (BeautifulSoup+LLM)
     ‚Üì                       ‚Üì                       ‚Üì
Lista de boletines ‚Üí Enlaces de documentos ‚Üí Texto completo
```

### Componentes Principales

#### 1. SIBOMScraper (Clase Principal)
- **Responsabilidad**: Orquestaci√≥n del proceso de scraping
- **Patr√≥n**: Facade + Strategy Pattern
- **Caracter√≠sticas**:
  - Manejo de rate limiting inteligente
  - Configuraci√≥n flexible de modelos LLM
  - Procesamiento secuencial y paralelo

#### 2. Pipeline de Procesamiento
```python
# Nivel 1: Extracci√≥n de listados
parse_listing_page() ‚Üí List[Dict]

# Nivel 2: Extracci√≥n de enlaces
parse_bulletin_content_links() ‚Üí List[str]

# Nivel 3: Extracci√≥n de contenido
parse_final_content() ‚Üí str
```

#### 3. Sistema de Archivos
- **Patr√≥n**: Repository Pattern
- **Estructura**:
  ```
  boletines/
  ‚îú‚îÄ‚îÄ {Ciudad}_{Numero}.json    # Archivos individuales
  ‚îú‚îÄ‚îÄ boletines.md              # √çndice markdown
  ‚îî‚îÄ‚îÄ boletines_index.json      # √çndice estructurado
  ```

#### 4. Utilidades de Soporte
- `indexar_boletines.py`: Generaci√≥n de √≠ndices
- `enrich_index_with_types.py`: Enriquecimiento de metadatos
- `comprimir_boletines.py`: Compresi√≥n para distribuci√≥n

### Flujo de Datos

```mermaid
graph TD
    A[URL Input] --> B{Tipo de URL}
    B -->|Bolet√≠n Individual| C[Extracci√≥n Directa]
    B -->|Listado| D[Detecci√≥n de Paginaci√≥n]
    D --> E[Procesamiento Multi-p√°gina]
    E --> F[Lista de Boletines]
    C --> F
    F --> G[Procesamiento Paralelo/Secuencial]
    G --> H[Extracci√≥n de Enlaces]
    H --> I[Extracci√≥n de Contenido]
    I --> J[Guardado Individual]
    J --> K[Actualizaci√≥n de √çndices]
```

## Convenciones de C√≥digo y Estilo

### Naming Conventions

#### Archivos y Directorios
- **Scripts principales**: `snake_case.py` (ej: `sibom_scraper.py`)
- **Utilidades**: `verbo_sustantivo.py` (ej: `indexar_boletines.py`)
- **Documentaci√≥n**: `MAYUSCULAS.md` (ej: `README.md`, `CHANGELOG.md`)
- **Configuraci√≥n**: `.lowercase` (ej: `.env`, `.gitignore`)

#### Clases y M√©todos
```python
# Clases: PascalCase
class SIBOMScraper:

# M√©todos p√∫blicos: snake_case
def parse_listing_page(self, html: str) -> List[Dict]:

# M√©todos privados: _snake_case
def _sanitize_filename(self, description: str) -> str:

# M√©todos de utilidad: _verbo_sustantivo
def _make_llm_call(self, prompt: str) -> str:
def _wait_for_rate_limit(self) -> None:
```

#### Variables y Constantes
```python
# Variables: snake_case
bulletin_url = "https://..."
content_links = []

# Constantes impl√≠citas: UPPER_CASE en contexto
BOLETINES_DIR = Path("boletines")
INDEX_FILE = Path("boletines_index.json")
```

### Organizaci√≥n del C√≥digo

#### Estructura de Archivos
```python
# 1. Shebang y docstring
#!/usr/bin/env python3
"""
Descripci√≥n del m√≥dulo
"""

# 2. Imports est√°ndar
import os
import sys
import json

# 3. Imports de terceros
import requests
from openai import OpenAI
from rich.console import Console

# 4. Imports locales (si aplica)
from .utils import helper_function

# 5. Configuraci√≥n global
load_dotenv()
console = Console()

# 6. Clases principales
class SIBOMScraper:
    # ...

# 7. Funciones de utilidad
def main():
    # ...

# 8. Entry point
if __name__ == '__main__':
    main()
```

#### Manejo de Errores
**Patr√≥n Consistente**: Try-Catch con Fallbacks Inteligentes
```python
try:
    # Intento principal (BeautifulSoup)
    result = fast_method()
except Exception as e:
    # Fallback con logging
    console.print(f"[yellow]‚ö† M√©todo r√°pido fall√≥, usando fallback[/yellow]")
    result = fallback_method()
```

#### Logging y Output
**Patr√≥n**: Rich Console con Niveles Sem√°nticos
```python
# Informaci√≥n: cyan
console.print("[cyan]üìã Procesando...[/cyan]")

# √âxito: green
console.print("[green]‚úì Completado[/green]")

# Advertencia: yellow
console.print("[yellow]‚ö† Advertencia[/yellow]")

# Error: red
console.print("[red]‚úó Error[/red]")

# Debug: dim
console.print("[dim]‚Üí Detalle t√©cnico[/dim]")
```

### Gesti√≥n de Configuraci√≥n

#### Variables de Entorno
```python
# Patr√≥n: Carga con fallbacks
api_key = args.api_key or os.getenv('OPENROUTER_API_KEY')
if not api_key:
    console.print("[bold red]Error: No se encontr√≥ OPENROUTER_API_KEY[/bold red]")
    sys.exit(1)
```

#### Configuraci√≥n de Argumentos
```python
# Patr√≥n: ArgumentParser con valores por defecto sensatos
parser.add_argument(
    '--model',
    type=str,
    default='google/gemini-3-flash-preview',  # Default inteligente
    help='Modelo de OpenRouter a usar'
)
```

## Patrones T√©cnicos y Preferencias

### Concurrencia y Paralelizaci√≥n

#### ThreadPoolExecutor Pattern
```python
if parallel > 1:
    with ThreadPoolExecutor(max_workers=parallel) as executor:
        futures = {executor.submit(self.process_bulletin, b, ...): b for b in bulletins}
        
        with Progress(...) as progress:
            for future in as_completed(futures):
                result = future.result()
                # Procesamiento de resultado
```

**Caracter√≠sticas**:
- Uso de `concurrent.futures` sobre `threading` directo
- Integraci√≥n con Rich Progress para UX
- Manejo robusto de excepciones en hilos

### Procesamiento de Datos

#### Hybrid Processing Pattern
```python
def parse_listing_page(self, html: str) -> List[Dict]:
    try:
        # Intento 1: BeautifulSoup (r√°pido, barato)
        soup = BeautifulSoup(html, 'lxml')
        # ... extracci√≥n nativa
        if bulletins:
            return bulletins
        else:
            raise ValueError("No se encontraron boletines")
    except Exception:
        # Fallback: LLM (lento, costoso, pero robusto)
        response = self._make_llm_call(prompt)
        # ... procesamiento LLM
```

**Ventajas del Patr√≥n**:
- **Costo optimizado**: 95% de casos usan BeautifulSoup (gratis)
- **Robustez**: LLM como fallback para casos complejos
- **Performance**: Prioriza velocidad cuando es posible

#### Rate Limiting Pattern
```python
def _wait_for_rate_limit(self):
    elapsed = time.time() - self.last_call_time
    if elapsed < self.rate_limit_delay:
        time.sleep(self.rate_limit_delay - elapsed)
    self.last_call_time = time.time()
```

### File I/O y Storage

#### Individual + Consolidated Pattern
```python
# 1. Guardar archivo individual
filepath = output_dir / f"{filename}.json"
with filepath.open('w', encoding='utf-8') as f:
    json.dump(result, f, indent=2, ensure_ascii=False)

# 2. Actualizar √≠ndice markdown
self._update_index_md(result, output_dir, base_url)

# 3. Mantener resumen consolidado
# (en main())
```

**Beneficios**:
- **Granularidad**: Acceso individual a boletines
- **Agregaci√≥n**: Resumen para an√°lisis masivo
- **Documentaci√≥n**: √çndice markdown para navegaci√≥n

#### Filename Sanitization Pattern
```python
def _sanitize_filename(self, description: str, number: str = None) -> str:
    # 1. Extraer n√∫mero del bolet√≠n
    number_match = re.search(r'(\d+)', number or description)
    
    # 2. Extraer nombre de ciudad
    city_match = re.search(r'(?:de\s+)([A-Z][a-zA-Z\s]+)', description)
    
    # 3. Limpiar caracteres especiales
    cleaned = re.sub(r'[^\w\s-]', '', city_name)
    
    # 4. Formato final
    return f"{cleaned}_{num}"
```

### API Integration Patterns

#### OpenRouter Integration
```python
# Patr√≥n: Client Wrapper con Configuraci√≥n Flexible
self.client = OpenAI(
    api_key=api_key,
    base_url="https://openrouter.ai/api/v1"  # Proxy pattern
)

# Uso con par√°metros din√°micos
params = {
    "model": self.model,  # Configurable
    "messages": [{"role": "user", "content": prompt}]
}

if use_json_mode:
    params["response_format"] = {"type": "json_object"}
```

#### HTTP Requests Pattern
```python
def fetch_html(self, url: str, max_retries: int = 3) -> str:
    for attempt in range(max_retries):
        try:
            response = requests.get(url, timeout=30)
            response.raise_for_status()
            return response.text
        except requests.RequestException as e:
            if attempt == max_retries - 1:
                raise
            time.sleep(2 ** attempt)  # Exponential backoff
```

### Testing y Validation

#### Validation Pattern
```python
# Validaci√≥n de entrada
if not html or len(html) < 100:
    raise ValueError(f"HTML inv√°lido ({len(html)} caracteres)")

# Validaci√≥n de salida
if len(text) < 100:
    raise ValueError(f"Texto extra√≠do muy corto ({len(text)} caracteres)")

# M√©tricas de calidad
ratio = text_size / html_size
if ratio < 0.05:
    console.print(f"[yellow]‚ö† Ratio texto/HTML sospechoso ({ratio:.1%})[/yellow]")
```

## Preferencias T√©cnicas Identificadas

### Bibliotecas y Frameworks Preferidos

#### Core Dependencies
```python
# Web scraping: BeautifulSoup + requests (no Selenium)
from bs4 import BeautifulSoup
import requests

# LLM: OpenAI client + OpenRouter (no llamadas directas)
from openai import OpenAI

# CLI UX: Rich (no argparse b√°sico)
from rich.console import Console
from rich.progress import Progress

# Concurrencia: concurrent.futures (no asyncio)
from concurrent.futures import ThreadPoolExecutor
```

#### Filosof√≠a de Dependencias
- **Minimalismo**: Solo 6 dependencias core
- **Estabilidad**: Bibliotecas maduras y bien mantenidas
- **Funcionalidad**: Cada dependencia resuelve un problema espec√≠fico
- **No over-engineering**: Evita frameworks pesados

### CLI Design Patterns

#### Progressive Disclosure
```python
# Informaci√≥n b√°sica siempre visible
console.print(Panel.fit(f"SIBOM Scraper\nModo: {mode}\nURL: {url}"))

# Detalles t√©cnicos solo en verbose/debug
console.print(f"[dim]‚Üí HTML recibido: {html_size:,} caracteres[/dim]")
```

#### User-Friendly Defaults
```python
# Valores por defecto sensatos
'--url': 'https://sibom.slyt.gba.gob.ar/cities/22'  # Carlos Tejedor
'--model': 'google/gemini-3-flash-preview'          # Balance calidad/costo
'--parallel': 1                                     # Conservador
```

#### Interactive vs Automated Modes
```python
# Modo interactivo por defecto
if filepath.exists() and not skip_existing:
    choice = input("¬øQu√© deseas hacer? (1-3) [1]: ")

# Modo autom√°tico con flag
if skip_existing:
    console.print("[yellow]‚è≠ Saltando autom√°ticamente[/yellow]")
```

### Performance Optimization

#### Lazy Loading Pattern
```python
# Solo carga p√°ginas cuando es necesario
if total_pages > 1:
    for page_num in range(2, total_pages + 1):
        # Carga bajo demanda
```

#### Caching Strategy
```python
# Evita re-procesamiento
if filepath.exists():
    # Reutiliza resultado existente
    with filepath.open('r') as f:
        return json.load(f)
```

#### Memory Management
```python
# Procesa uno a la vez en modo secuencial
for bulletin in bulletins:
    result = self.process_bulletin(bulletin, ...)
    # Resultado se guarda inmediatamente, no se acumula en memoria
```

### Documentation y Maintenance

#### Self-Documenting Code
```python
def _sanitize_filename(self, description: str, number: str = None) -> str:
    """
    Convierte descripci√≥n en nombre de archivo v√°lido.
    Ejemplo: "105¬∫ de Carlos Tejedor" -> "Carlos_Tejedor_105"
    """
```

#### Comprehensive Documentation
- **README.md**: Gu√≠a de usuario completa con ejemplos
- **MODELOS.md**: Comparaci√≥n t√©cnica detallada
- **CHANGELOG.md**: Historial de cambios versionado
- **FEATURES.md**: Documentaci√≥n de caracter√≠sticas espec√≠ficas

#### Maintenance Scripts
```python
# Scripts de utilidad incluidos
indexar_boletines.py      # Generaci√≥n de √≠ndices
enrich_index_with_types.py # Enriquecimiento de datos
comprimir_boletines.py    # Optimizaci√≥n para distribuci√≥n
comparar_modelos.sh       # Testing de calidad
```

## Correctness Properties

*A property is a characteristic or behavior that should hold true across all valid executions of a system-essentially, a formal statement about what the system should do. Properties serve as the bridge between human-readable specifications and machine-verifiable correctness guarantees.*

### Property 1: Hybrid Processing Fallback
*For any* HTML input, if BeautifulSoup parsing fails, the system should automatically fall back to LLM processing and return valid results
**Validates: Requirements 1.1, 1.4**

### Property 2: File Naming Consistency
*For any* bulletin description, the sanitized filename should be deterministic and follow the pattern `{Ciudad}_{Numero}.json`
**Validates: Requirements 2.2, 3.3**

### Property 3: Rate Limiting Compliance
*For any* sequence of LLM calls, the time between consecutive calls should be at least `rate_limit_delay` seconds
**Validates: Requirements 3.4**

### Property 4: Data Integrity Preservation
*For any* processed bulletin, the individual JSON file and consolidated index should contain consistent metadata
**Validates: Requirements 3.3, 4.4**

### Property 5: Parallel Processing Safety
*For any* parallel execution with N workers, the final result should be identical to sequential processing (order-independent)
**Validates: Requirements 3.1**

### Property 6: Configuration Validation
*For any* invalid configuration (missing API key, invalid model), the system should fail fast with clear error messages
**Validates: Requirements 4.2, 4.5**

## Error Handling

El sistema implementa una estrategia de **graceful degradation** con m√∫ltiples niveles de fallback:

1. **Nivel de Parsing**: BeautifulSoup ‚Üí LLM fallback
2. **Nivel de Red**: Reintentos con backoff exponencial
3. **Nivel de Archivo**: Verificaci√≥n de existencia antes de procesamiento
4. **Nivel de Usuario**: Modo interactivo vs autom√°tico

## Testing Strategy

### Dual Testing Approach
- **Unit tests**: Validaci√≥n de funciones de utilidad (sanitizaci√≥n, rate limiting)
- **Property tests**: Verificaci√≥n de invariantes del sistema (consistencia de datos, idempotencia)

### Property-Based Testing Configuration
- M√≠nimo 100 iteraciones por test de propiedad
- Framework recomendado: `hypothesis` para Python
- Tags de referencia: **Feature: python-cli-analysis, Property {N}: {descripci√≥n}**

### Testing Priorities
1. **Cr√≠tico**: Sanitizaci√≥n de nombres de archivo, rate limiting
2. **Importante**: Consistencia de datos, manejo de errores
3. **Deseable**: Performance, UX flows

Este an√°lisis revela un proyecto maduro con decisiones arquitect√≥nicas s√≥lidas, convenciones consistentes, y un enfoque pragm√°tico hacia la resoluci√≥n de problemas complejos de web scraping y procesamiento de datos.