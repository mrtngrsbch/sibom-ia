# Estrategia de Bypass del LLM - Ahorro Masivo de Tokens

## üéØ Objetivo

Reducir el consumo de tokens del LLM respondiendo queries simples **DIRECTAMENTE desde el √≠ndice JSON** sin necesidad de procesamiento con IA.

## üìä Problema Identificado

### Antes de la Optimizaci√≥n
- **Query:** "decretos carlos tejedor de 2025"
- **Tokens de entrada:** 60,625 tokens ($0.18)
- **Tokens de salida:** 48 tokens ($0.0007)
- **Total:** $0.1826 por query
- **Problema:** Enviamos TODO el contexto al LLM solo para que genere un resumen de 2 l√≠neas

### An√°lisis del Desperdicio
- El LLM recibe 500 documentos completos en el contexto
- Solo genera: "Se encontraron 500 decretos de Carlos Tejedor del a√±o 2025..."
- **95% de los tokens son desperdicio** - el LLM no necesita leer los documentos para contar

## üí° Soluci√≥n Implementada

### Clasificador de Intenci√≥n de Query

**Archivo:** `chatbot/src/lib/query-intent-classifier.ts`

Clasifica queries en 10 categor√≠as:

#### ‚úÖ Queries SIN LLM (respuesta directa desde JSON)

1. **`simple-listing`** - Listados simples
   - Ejemplos: "decretos de carlos tejedor 2025", "ordenanzas de merlo"
   - Respuesta: Contar en √≠ndice + generar template
   - Ahorro: ~60,000 tokens

2. **`count`** - Conteos
   - Ejemplos: "cu√°ntas ordenanzas hay en merlo", "cantidad de decretos 2025"
   - Respuesta: Contar en √≠ndice
   - Ahorro: ~60,000 tokens

3. **`search-by-number`** - B√∫squeda por n√∫mero
   - Ejemplos: "ordenanza 2947 de carlos tejedor", "decreto 123"
   - Respuesta: Buscar en √≠ndice por n√∫mero
   - Ahorro: ~60,000 tokens

4. **`latest`** - √öltima normativa
   - Ejemplos: "√∫ltima ordenanza de merlo", "decreto m√°s reciente"
   - Respuesta: Ordenar por fecha + tomar primera
   - Ahorro: ~60,000 tokens

5. **`off-topic`** - Fuera de tema
   - Ejemplos: "c√≥mo est√° el clima", "receta de pizza"
   - Respuesta: Template pre-definido
   - Ahorro: ~60,000 tokens

#### ‚ùå Queries CON LLM (requieren procesamiento)

6. **`content-analysis`** - An√°lisis de contenido
   - Ejemplos: "qu√© dice la ordenanza 2947 sobre donaciones"
   - Requiere: Leer contenido + analizar
   - Tokens: Normal (~4,000)

7. **`semantic-search`** - B√∫squeda sem√°ntica
   - Ejemplos: "ordenanzas relacionadas con tr√°nsito"
   - Requiere: BM25 + ranking sem√°ntico
   - Tokens: Normal (~4,000)

8. **`comparison`** - Comparaciones
   - Ejemplos: "diferencias entre ordenanza X y Y"
   - Requiere: Leer ambos documentos + comparar
   - Tokens: Alto (~8,000)

9. **`faq`** - Preguntas frecuentes
   - Ejemplos: "qu√© municipios hay disponibles", "c√≥mo funciona"
   - Requiere: LLM econ√≥mico (Gemini Flash)
   - Tokens: Bajo (~500)

10. **`date-range`** - Rango de fechas (futuro)
    - Ejemplos: "ordenanzas de enero 2025"
    - Actualmente: Filtro + listado simple
    - Tokens: 0 (respuesta directa)

### Flujo de Decisi√≥n

```
Usuario hace query
    ‚Üì
classifyQueryIntent(query)
    ‚Üì
¬øneedsLLM = false?
    ‚Üì S√ç
generateDirectResponse()
    ‚Üì
Devolver respuesta + fuentes
    ‚Üì
0 tokens consumidos ‚úÖ
    
    ‚Üì NO
Cargar contexto RAG
    ‚Üì
Llamar LLM
    ‚Üì
~4,000-60,000 tokens ‚ùå
```

## üìà M√©tricas de Ahorro

### Queries Comunes y su Ahorro

| Query | Antes | Despu√©s | Ahorro |
|-------|-------|---------|--------|
| "decretos carlos tejedor 2025" | 60,625 tokens ($0.18) | 0 tokens ($0.00) | **100%** |
| "cu√°ntas ordenanzas hay en merlo" | 60,625 tokens ($0.18) | 0 tokens ($0.00) | **100%** |
| "ordenanza 2947" | 4,000 tokens ($0.012) | 0 tokens ($0.00) | **100%** |
| "√∫ltima ordenanza de merlo" | 4,000 tokens ($0.012) | 0 tokens ($0.00) | **100%** |
| "qu√© dice la ordenanza 2947 sobre X" | 4,000 tokens ($0.012) | 4,000 tokens ($0.012) | 0% (necesita LLM) |

### Proyecci√≥n de Ahorro Mensual

Asumiendo 1,000 queries/mes:

**Distribuci√≥n estimada:**
- 60% queries simples (listados, conteos) ‚Üí 600 queries
- 20% b√∫squedas espec√≠ficas ‚Üí 200 queries
- 15% an√°lisis de contenido ‚Üí 150 queries
- 5% FAQ ‚Üí 50 queries

**Antes:**
- 600 √ó $0.18 = $108.00 (listados masivos)
- 200 √ó $0.012 = $2.40 (b√∫squedas)
- 150 √ó $0.012 = $1.80 (an√°lisis)
- 50 √ó $0.0007 = $0.035 (FAQ)
- **Total: $112.24/mes**

**Despu√©s:**
- 600 √ó $0.00 = $0.00 (bypass LLM) ‚úÖ
- 200 √ó $0.00 = $0.00 (bypass LLM) ‚úÖ
- 150 √ó $0.012 = $1.80 (necesita LLM)
- 50 √ó $0.0007 = $0.035 (FAQ econ√≥mico)
- **Total: $1.84/mes**

**Ahorro: $110.40/mes (98.4%)**

## üîß Implementaci√≥n T√©cnica

### 1. Clasificador de Intenci√≥n

```typescript
// chatbot/src/lib/query-intent-classifier.ts
export function classifyQueryIntent(query: string): QueryIntentResult {
  // Detecta patrones en la query
  // Retorna: { intent, needsLLM, confidence, reason }
}
```

### 2. Generador de Respuestas Directas

```typescript
export function generateDirectResponse(
  intent: QueryIntent,
  sources: any[],
  filters: { municipality?, type?, year? }
): string {
  // Genera respuesta usando templates
  // Sin llamar al LLM
}
```

### 3. Integraci√≥n en API Route

```typescript
// chatbot/src/app/api/chat/route.ts

// Clasificar intenci√≥n
const intentResult = classifyQueryIntent(query);

// Si NO necesita LLM, bypass
if (!intentResult.needsLLM) {
  const directResponse = generateDirectResponse(
    intentResult.intent,
    retrievedContext.sources,
    filters
  );
  
  // Devolver respuesta directa (0 tokens)
  return streamDirectResponse(directResponse, sources);
}

// Si necesita LLM, continuar normal
const result = streamText({ model, system, messages });
```

### 4. Mejora en UI - Contador de Fuentes

```typescript
// chatbot/src/components/chat/Citations.tsx
<h4>
  {sources.length} Fuentes Consultadas
</h4>
```

## üß™ Testing

### Casos de Prueba

1. **Listado masivo**
   - Query: "decretos carlos tejedor de 2025"
   - Esperado: 0 tokens, respuesta directa
   - Verificar: Log muestra "BYPASS LLM"

2. **Conteo**
   - Query: "cu√°ntas ordenanzas hay en merlo"
   - Esperado: 0 tokens, n√∫mero exacto
   - Verificar: Respuesta sin llamar LLM

3. **B√∫squeda por n√∫mero**
   - Query: "ordenanza 2947"
   - Esperado: 0 tokens, documento espec√≠fico
   - Verificar: Bypass LLM

4. **An√°lisis de contenido (necesita LLM)**
   - Query: "qu√© dice la ordenanza 2947 sobre donaciones"
   - Esperado: ~4,000 tokens, an√°lisis detallado
   - Verificar: LLM se llama normalmente

### Logs de Verificaci√≥n

```
[ChatAPI] üéØ Intenci√≥n detectada: simple-listing (confidence: 0.95, needsLLM: false)
[ChatAPI] üìù Raz√≥n: Listado simple - respuesta directa desde √≠ndice
[ChatAPI] üöÄ BYPASS LLM - Generando respuesta directa
[ChatAPI] ‚úÖ Respuesta directa generada (0 tokens LLM)
[ChatAPI] üí∞ Ahorro estimado: ~60,000 tokens (~$0.18)
```

## üìã Checklist de Implementaci√≥n

- [x] ‚úÖ Crear clasificador de intenci√≥n (`query-intent-classifier.ts`)
- [x] ‚úÖ Implementar generador de respuestas directas
- [x] ‚úÖ Integrar bypass en API route
- [x] ‚úÖ Agregar contador de fuentes en UI
- [x] ‚úÖ Logging detallado para debugging
- [ ] ‚è≥ Tests unitarios para clasificador
- [ ] ‚è≥ Tests de integraci√≥n para bypass
- [ ] ‚è≥ Monitoreo de ahorro real en producci√≥n
- [ ] ‚è≥ Dashboard de m√©tricas de uso

## üöÄ Pr√≥ximos Pasos

### Fase 2: Optimizaciones Adicionales

1. **Cache de respuestas frecuentes**
   - Guardar queries comunes en localStorage
   - Evitar incluso la b√∫squeda en √≠ndice
   - Ahorro adicional: latencia

2. **√çndice SQL.js (opcional)**
   - Migrar √≠ndice JSON a SQLite en memoria
   - Queries SQL m√°s r√°pidas
   - Mejor para datasets >100MB

3. **Prefetching inteligente**
   - Precargar municipios populares
   - Anticipar queries comunes
   - Mejor UX

4. **Compresi√≥n de √≠ndice**
   - Usar MessagePack en vez de JSON
   - Reducir tama√±o de descarga
   - Mejor performance en m√≥viles

### Fase 3: Analytics y Monitoreo

1. **Dashboard de m√©tricas**
   - Queries por tipo de intenci√≥n
   - Ahorro de tokens en tiempo real
   - Queries m√°s frecuentes

2. **A/B Testing**
   - Comparar bypass vs LLM siempre
   - Medir satisfacci√≥n del usuario
   - Optimizar clasificador

## üìö Referencias

- **Clasificador:** `chatbot/src/lib/query-intent-classifier.ts`
- **API Route:** `chatbot/src/app/api/chat/route.ts`
- **UI Citations:** `chatbot/src/components/chat/Citations.tsx`
- **Documentaci√≥n anterior:** `FIX_MASSIVE_LISTINGS.md`

## üéì Lecciones Aprendidas

1. **No todo necesita IA**: El 80% de las queries son simples y se pueden responder con l√≥gica b√°sica
2. **Los datos estructurados son oro**: Nuestros JSON tienen toda la info necesaria
3. **El LLM es caro**: 60,000 tokens para decir "hay 500 decretos" es un desperdicio
4. **La clasificaci√≥n temprana es clave**: Detectar la intenci√≥n ANTES de cargar contexto
5. **Los templates son suficientes**: Para queries simples, un template bien hecho es mejor que el LLM

## üí¨ Feedback del Usuario

> "60,625 tokens es una locura! Debemos pensar en otra estrategia que no consuma tokens."

**Soluci√≥n implementada:** Bypass completo del LLM para queries simples.

**Resultado:** 98.4% de ahorro en costos mensuales estimados.
