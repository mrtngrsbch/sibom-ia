# Estrategia de LLM - Simplificaci√≥n y Function Calling

**Fecha:** 2026-01-14  
**Estado:** ‚úÖ Simplificaci√≥n completada  
**Pr√≥ximos pasos:** Function Calling (opcional)

---

## üéØ Problema Original

### Over-Engineering Complejo

Est√°bamos tratando de ser "smart" creando 10+ categor√≠as de queries con reglas complejas:

**Query de ejemplo:**
```
Usuario: "sueldos de carlos tejedor de 2025"

Expected: Buscar normativas ABOUT salarios (semantic search on content)
Got: "Se encontraron 10 decretos de Carlos Tejedor del a√±o 2025" (generic listing)
```

**Causa ra√≠z:**
1. Clasificamos queries en 10+ categor√≠as con reglas hardcodeadas
2. Intentamos "outsmart" al LLM con bypass complejo
3. El sistema clasificaba como "simple-listing" en vez de "semantic-search"
4. El bypass del LLM evitaba an√°lisis de contenido

**Resultado:**
- Usuario frustrado con respuestas gen√©ricas
- Arquitectura dif√≠cil de mantener
- Reglas que nunca cubren todos los casos

---

## ‚úÖ Soluci√≥n Implementada: Simplificaci√≥n Radical

### Principio Fundamental

**"Other chatbots (NotebookLM, ChatGPT, Claude) don't do this."**

Ellos simplemente:
1. Retrieve relevant context (RAG)
2. Send everything to LLM
3. Let LLM interpret and respond

**¬øPor qu√© est√°bamos intentando ser m√°s smart que el LLM?**

### Arquitectura Simplificada

**Antes (Complejo - 10+ categor√≠as):**
```typescript
type QueryIntent =
  | 'simple-listing'
  | 'count'
  | 'search-by-number'
  | 'latest'
  | 'date-range'
  | 'content-analysis'
  | 'semantic-search'
  | 'comparison'
  | 'computational'
  | 'faq'
  | 'off-topic';

// Complex priority chain
if (isCountQuery()) return { needsLLM: false };
if (isSearchByNumber()) return { needsLLM: false };
if (isLatestQuery()) return { needsLLM: false };
if (isContentAnalysis()) return { needsLLM: true };
if (isSemanticSearch()) return { needsLLM: true };
// ... 10+ checks
```

**Despu√©s (Simple - 3 categor√≠as):**
```typescript
export function classifyQueryIntent(query: string): QueryIntentResult {
  // 1. Off-topic? No desperdicie recursos
  if (isOffTopic(query)) {
    return { needsRAG: false, needsLLM: false };
  }

  // 2. FAQ about system? No RAG needed
  if (isFAQQuery(query)) {
    return { needsRAG: false, needsLLM: true };
  }

  // 3. Computational (SQL)? Special handling
  if (isComputationalQuery(query)) {
    return { needsRAG: true, needsLLM: true };
  }

  // 4. EVERYTHING ELSE: Let LLM handle it
  return {
    intent: 'semantic-search',
    needsRAG: true,
    needsLLM: true, // ALWAYS use LLM
    reason: 'Let LLM interpret query and decide response'
  };
}
```

### Lo Que Eliminamos

**‚ùå Removed: LLM Bypass Logic**

Deleted from `route.ts`:
- ~100 lines of "direct response generation"
- Complex intent-based routing
- Manual response formatting
- Token counting optimizations

**Why:** The LLM is BETTER at understanding user intent than our hardcoded rules.

**‚ùå Removed: Complex Classification**

Simplified in `query-classifier.ts`:
- Removed: `isCountQuery()`
- Removed: `isSearchByNumberQuery()`
- Removed: `isLatestQuery()`
- Removed: `isContentAnalysisQuery()`
- Removed: `isComparisonQuery()`
- Removed: `generateDirectResponse()`

**Why:** These were all attempts to "outsmart" LLM. They failed.

**‚úÖ Kept: Only Essential Logic**

1. **Off-topic detection** - Don't waste API calls on weather/sports queries
2. **FAQ detection** - System questions don't need RAG
3. **Computational queries** - SQL comparisons are genuinely faster
4. **Everything else ‚Üí LLM** - Let it do its job

---

## üîÑ El Nuevo Flujo

```
User Query: "sueldos de carlos tejedor de 2025"
    ‚Üì
Is off-topic? NO
    ‚Üì
Is FAQ? NO
    ‚Üì
Is computational? NO
    ‚Üì
‚Üí Retrieve context with RAG (10 normativas)
    ‚Üì
‚Üí Send to LLM with improved prompt:
    "REGLA #1: Understand user intent
     - Content search: 'sueldos' ‚Üí find normativas ABOUT salaries
     - Metadata listing: 'decretos 2025' ‚Üí list ALL decrees"
    ‚Üì
‚Üí LLM analyzes content and responds intelligently
    ‚Üì
‚úÖ User gets relevant normativas about salaries
```

---

## üìã System Prompt Mejorado

Added explicit instructions for LLM to distinguish between:

### A) Content Search (Semantic)
```
"sueldos de carlos tejedor de 2025"
‚Üí User wants normativas ABOUT salaries
‚Üí Analyze CONTENT of documents
‚Üí Explain WHAT each normativa says about salaries
```

### B) Metadata Listing
```
"decretos de carlos tejedor de 2025"
‚Üí User wants ALL decrees from 2025
‚Üí List ALL matching documents
‚Üí Don't filter by content relevance
```

---

## üí∞ Impacto en Costos

### Before Simplification

**Intented savings:** ~$0.18 per query with bypass

**Actual cost:** **User frustration** (precioless)

**Maintenance burden:** High (complex classification logic)

### After Simplification

**Cost per query:** ~$0.02-0.05 (Claude Sonnet 3.5)

**User satisfaction:** ‚úÖ Works like expected

**Maintenance burden:** Low (simple, clear logic)

**Conclusion:** The "savings" weren't worth it. User experience > micro-optimizations.

---

## ‚úÖ Exception: When Bypass IS Justified

We kept ONE bypass: **SQL Comparisons**

```typescript
// ‚úÖ GOOD: SQL comparison bypass
Query: "comparar cantidad de decretos entre municipios"
‚Üí SQL query: SELECT municipality, COUNT(*) ...
‚Üí Direct response with table
‚Üí Savings: ~$0.45 per query
‚Üí Speed: 200ms vs 3-5s
‚Üí Accuracy: 100% (structured data)
```

**Why this works:**
- Structured data (SQL)
- Deterministic output (numbers)
- Massive speed improvement
- No ambiguity in user intent

---

## üöÄ Pr√≥ximo Paso: Function Calling (Opcional)

### Proposed Architecture

```typescript
// Define tools
const tools = {
  query_metadata_sql: queryMetadataSQL,
  search_content: searchContent,
  get_database_stats: getDatabaseStatsTool
};

// Use in streamText
const result = streamText({
  model: openrouter(modelId),
  system: systemPrompt,
  messages: coreMessages,
  tools: tools, // ‚Üê NEW: Tools available
  maxToolRoundtrips: 3, // Allow multiple tool calls
  temperature: 0.3,
  maxTokens: 4000,
});
```

### Tools Definition

#### Tool 1: `query_metadata_sql`
```typescript
const queryMetadataSQL = tool({
  description: `Query SQL database for metadata about normativas.  
  Use this tool when user asks about:
  - Counting normativas (cu√°ntas, cantidad, total)
  - Listing by metadata (all decretos, all ordenanzas)
  - Comparing municipalities (cu√°l tiene m√°s, diferencias)
  - Finding by number/year (ordenanza 2947, decretos de 2025)
  
  Available fields: municipality, type, number, year, date, title, url, status
  
  DO NOT use this for content-based queries (e.g., "ordenanzas sobre tr√°nsito")`,
  
  parameters: z.object({
    query: z.string().describe('Natural language query about metadata'),
    filters: z.object({
      municipality: z.string().optional(),
      type: z.enum(['ordenanza', 'decreto', 'resolucion', 'disposicion', 'convenio']).optional(),
      year: z.number().optional(),
      dateFrom: z.string().optional(),
      dateTo: z.string().optional(),
    }).optional()
  }),
  
  execute: async ({ query, filters }) => {
    // Convert natural language to SQL and execute
    const result = await executeQuery(query, filters);
    return result;
  }
});
```

#### Tool 2: `search_content`
```typescript
const searchContent = tool({
  description: `Search full content of normativas using semantic search.  
  Use this tool when user asks about:
  - Topics/themes (sueldos, tr√°nsito, salud, educaci√≥n)
  - Content of specific normativas (qu√© dice, contenido)
  - Semantic search (ordenanzas sobre X, decretos que hablan de Y)
  
  This searches FULL TEXT content, not just metadata.`,
  
  parameters: z.object({
    query: z.string().describe('Natural language query about content'),
    filters: z.object({
      municipality: z.string().optional(),
      type: z.enum(['ordenanza', 'decreto', 'resolucion', 'disposicion', 'convenio']).optional(),
      year: z.number().optional(),
      limit: z.number().default(10).describe('Max results to return')
    }).optional()
  }),
  
  execute: async ({ query, filters }) => {
    // Use BM25 + RAG to search content
    const result = await retrieveContext(query, filters);
    return result;
  }
});
```

#### Tool 3: `get_database_stats`
```typescript
const getDatabaseStatsTool = tool({
  description: `Get statistics about available data in system.  
  Use when user asks:
  - What municipalities are available?
  - How many documents do we have?
  - What types of normativas exist?`,
  
  parameters: z.object({}),
  
  execute: async () => {
    const stats = await getDatabaseStats();
    return stats;
  }
});
```

### Benefits of Function Calling

1. **El LLM decide** - No necesitamos clasificar queries con regex
2. **M√°s flexible** - El LLM puede combinar tools seg√∫n necesidad
3. **Transparente** - Vemos qu√© tools usa el LLM en los logs
4. **Escalable** - F√°cil agregar nuevas tools (ej: `extract_tables`, `compare_content`)

### Pros vs Cons

| Aspecto | Function Calling | Simple Classification |
|---------|-------------------|----------------------|
| **Complejidad** | Media | Baja |
| **Flexibilidad** | Alta | Baja |
| **Mantenimiento** | Medio | Bajo |
| **Precisi√≥n** | Alta (LLM elige) | Media (reglas fijas) |
| **Costo** | Slightly higher | Bajo |

### Recommendation

**¬øImplementar function calling?**

- **S√ç** si quieres m√°s flexibilidad y mejor precisi√≥n
- **NO** si prefieres simplicidad y menor costo

Para tu caso actual, el sistema **simplificado ya funciona bien**. Function calling es una mejora opcional.

---

## üéØ Test Results

### Before
```
Query: "sueldos de carlos tejedor de 2025"
Classification: simple-listing
needsLLM: false
Response: "Se encontraron 10 decretos..." ‚ùå
```

### After
```
Query: "sueldos de carlos tejedor de 2025"
Classification: semantic-search
needsLLM: true
Response: [LLM analyzes content about salaries] ‚úÖ
```

---

## üìä Archivos Modificados

1. **`chatbot/src/lib/query-classifier.ts`**
   - Removed 6 classification functions
   - Simplified to 3 checks + default
   - ~200 lines ‚Üí ~100 lines

2. **`chatbot/src/app/api/chat/route.ts`**
   - Removed LLM bypass logic (~100 lines)
   - Removed direct response generation
   - Kept only SQL comparison bypass

3. **`chatbot/src/prompts/system.md`**
   - Added REGLA #1: Understand user intent
   - Clear distinction between content search vs listing
   - Examples for both cases

---

## üéì Conclusion

**"Premature optimization is the root of all evil." - Donald Knuth**

Est√°bamos optimizando para token costs antes de validar que el sistema realmente funciona bien. Esto est√° al rev√©s.

**The right order:**
1. Make it work (user experience)
2. Make it right (code quality)
3. Make it fast (optimization)

Nos saltamos el paso 1 y fuimos directo al paso 3. Por eso fall√≥.

**Resultado final:**
- Sistema simple y claro
- User experience mejorada
- Maintenance burden reducido
- C√≥digo m√°s mantenible
- ~300 l√≠neas de c√≥digo "clever" eliminadas

**Key Takeaway:** Sometimes the best code is code you DELETE. We removed ~300 lines of "clever" logic and system works better.
