# üöÄ Pr√≥ximos Pasos - SIBOM Scraper Assistant

## ‚úÖ Lo que acabamos de implementar

He implementado exitosamente la **Opci√≥n H√≠brida (GitHub + Vercel con cache agresivo y soporte gzip)**. Esto te permite:

1. ‚úÖ **Retriever h√≠brido** (`chatbot/src/lib/rag/retriever.ts`)
   - Soporta datos locales O GitHub Raw
   - Cache multi-nivel (1 hora √≠ndice, 30 min archivos)
   - Soporte para archivos comprimidos con gzip
   - Ahorro de 80% bandwidth con compresi√≥n

2. ‚úÖ **Script de compresi√≥n** (`python-cli/comprimir_boletines.py`)
   - Comprime 533 MB ‚Üí ~100 MB
   - Interfaz interactiva con progreso
   - Opci√≥n de mantener originales

3. ‚úÖ **Configuraci√≥n actualizada** (`.env.example`)
   - Variables para GitHub Raw
   - Documentaci√≥n inline
   - Opciones de compresi√≥n

4. ‚úÖ **Documentaci√≥n completa**
   - Gu√≠a de deployment paso a paso
   - README para repo de datos
   - An√°lisis de costos y bandwidth

---

## üìã Para Deployar a Producci√≥n

### Paso 1: Comprimir Datos (5 minutos)

```bash
cd python-cli
python comprimir_boletines.py
```

**Resultado**: Archivos `.json.gz` listos para GitHub

---

### Paso 2: Crear Repo de Datos en GitHub (10 minutos)

1. **Crear repositorio p√∫blico**:
   ```
   Nombre: sibom-data
   Visibilidad: Public
   No inicializar con README
   ```

2. **Subir datos**:
   ```bash
   git clone https://github.com/TU-USUARIO/sibom-data.git
   cd sibom-data

   # Copiar archivos comprimidos
   cp ../sibom-scraper-assistant/python-cli/boletines/*.json.gz ./boletines/
   cp ../sibom-scraper-assistant/python-cli/boletines_index.json.gz ./

   # Usar el README que cre√©
   cp ../sibom-scraper-assistant/docs/SIBOM_DATA_REPO_README.md ./README.md

   # Commit y push
   git add .
   git commit -m "Initial commit: Add compressed bulletins data"
   git push origin main
   ```

3. **Verificar acceso**:
   ```
   https://raw.githubusercontent.com/TU-USUARIO/sibom-data/main/boletines_index.json.gz
   ```

   Debe descargarse el archivo.

---

### Paso 3: Configurar Variables Locales (2 minutos)

Edita `chatbot/.env.local`:

```bash
# API de OpenRouter
OPENROUTER_API_KEY=sk-or-v1-tu-clave-aqu√≠

# Modelo econ√≥mico (10x m√°s barato que Claude)
ANTHROPIC_MODEL=google/gemini-3-flash-preview

# GitHub Data
GITHUB_DATA_REPO=TU-USUARIO/sibom-data
GITHUB_DATA_BRANCH=main
GITHUB_USE_GZIP=true

# App
NEXT_PUBLIC_APP_URL=http://localhost:3000
```

---

### Paso 4: Probar Localmente (5 minutos)

```bash
cd chatbot
npm install
npm run dev
```

Abre [http://localhost:3000](http://localhost:3000) y prueba:

```
Pregunta: "ordenanza de presupuesto en Carlos Tejedor"
```

**Logs esperados**:
```
[RAG] üì• Descargando √≠ndice desde GitHub: https://raw.githubusercontent.com/...
[RAG] ‚úÖ √çndice descargado: 3210 documentos (gzip)
[RAG] Query completada en 450ms
[RAG] Recuperados 5 documentos relevantes
[RAG] Cache: 5 archivos en memoria
```

Si ves esto, ¬°funciona! üéâ

---

### Paso 5: Deploy a Vercel (10 minutos)

1. **Ir a [vercel.com](https://vercel.com)**

2. **New Project** ‚Üí Importar `sibom-scraper-assistant`

3. **Configurar Build**:
   - Root Directory: `chatbot`
   - Build Command: `npm run build` (default)
   - Output Directory: `.next` (default)

4. **Environment Variables** (copiar de `.env.local`):

   | Variable | Value |
   |----------|-------|
   | `OPENROUTER_API_KEY` | `sk-or-v1-...` |
   | `ANTHROPIC_MODEL` | `google/gemini-3-flash-preview` |
   | `GITHUB_DATA_REPO` | `TU-USUARIO/sibom-data` |
   | `GITHUB_DATA_BRANCH` | `main` |
   | `GITHUB_USE_GZIP` | `true` |
   | `NEXT_PUBLIC_APP_URL` | (Vercel te lo dar√°) |
   | `NODE_ENV` | `production` |

5. **Deploy**

6. **Actualizar `NEXT_PUBLIC_APP_URL`**:
   - Vercel te da una URL: `https://sibom-chatbot-abc123.vercel.app`
   - Actualiza la variable en Vercel Settings ‚Üí Environment Variables
   - Redeploy (opcional)

---

## üìä Monitoreo y Optimizaci√≥n

### Verificar Funcionamiento

1. **Abrir app en Vercel**: `https://tu-app.vercel.app`
2. **Hacer consulta de prueba**
3. **Ver logs** en Vercel ‚Üí Functions ‚Üí Logs

### Monitorear Bandwidth

**GitHub**:
- Settings ‚Üí Insights ‚Üí Traffic
- L√≠mite: 100 GB/mes (gratis)

**Vercel**:
- Dashboard ‚Üí Analytics ‚Üí Bandwidth
- L√≠mite: 100 GB/mes (gratis)

### Costos Estimados

**Con Gemini 3 Flash** (100 consultas/d√≠a):
- Bandwidth GitHub: ~300 MB/mes (con gzip)
- Bandwidth Vercel: ~300 MB/mes
- LLM: ~$0.15/mes

**Total: ~$0.15/mes** (pr√°cticamente gratis)

---

## üîß Optimizaciones Futuras

### 1. Pre-warming de Cache (Opcional)

Crea `/chatbot/vercel.json`:

```json
{
  "crons": [{
    "path": "/api/healthcheck",
    "schedule": "0 * * * *"
  }]
}
```

Esto mantiene el cache caliente cada hora.

### 2. Token de GitHub (Aumentar L√≠mites)

Si llegas a rate limits:
1. Genera token: https://github.com/settings/tokens
2. Permisos: `public_repo` (read)
3. Agrega a Vercel: `GITHUB_TOKEN=ghp_xxx...`

### 3. Cambiar a Modelo Gratuito

Si quieres 100% gratis:

```bash
ANTHROPIC_MODEL=zhipu/glm-4.5-air:free
```

(Con l√≠mites, pero suficiente para MVP)

---

## üìö Documentaci√≥n Completa

He creado estos archivos para ti:

1. **`docs/DEPLOYMENT_GITHUB_VERCEL.md`**
   - Gu√≠a completa de deployment
   - Troubleshooting
   - Optimizaciones avanzadas

2. **`docs/SIBOM_DATA_REPO_README.md`**
   - README para tu repo `sibom-data`
   - Descripci√≥n de estructura
   - Gu√≠a de uso

3. **`python-cli/comprimir_boletines.py`**
   - Script de compresi√≥n con interfaz
   - Estad√≠sticas de ahorro

4. **`chatbot/.env.example`**
   - Todas las variables documentadas
   - Ejemplos de configuraci√≥n

---

## ‚ùì Preguntas Respondidas

### 1. ¬øPor qu√© tengo configs de Anthropic si solo uso OpenRouter?

**Respuesta**: Por compatibilidad hist√≥rica del SDK. El c√≥digo usa `ANTHROPIC_BASE_URL=https://openrouter.ai/api/v1` para redirigir todo a OpenRouter.

**D√≥nde cambiar modelo**: Variable `ANTHROPIC_MODEL` en `.env.local`

### 2. ¬øQu√© tecnolog√≠as usa el RAG?

**Respuesta**: RAG sin embeddings (keyword-based):
- √çndice JSON de metadatos (790 KB)
- B√∫squeda por scoring heur√≠stico
- Cache en memoria (1 hora √≠ndice, 30 min archivos)
- Lazy loading (solo lee 5 de 3,210 archivos)

### 3. ¬øVariables de entorno para Vercel?

**Respuesta**:
```bash
OPENROUTER_API_KEY      # Requerida
ANTHROPIC_MODEL         # Opcional (default: claude-3.5-sonnet)
GITHUB_DATA_REPO        # Requerida para GitHub Raw
GITHUB_DATA_BRANCH      # Opcional (default: main)
GITHUB_USE_GZIP         # Opcional (default: false)
NEXT_PUBLIC_APP_URL     # URL de Vercel
```

### 4. ¬øD√≥nde est√° la l√≥gica del comportamiento del chat?

**Respuesta**:
- **System Prompt**: `/chatbot/src/prompts/system.md`
- **Par√°metros LLM**: `/chatbot/src/app/api/chat/route.ts` (temperature: 0.3, maxTokens: 2000)
- **Recuperaci√≥n RAG**: `/chatbot/src/lib/rag/retriever.ts` (limit: 5 docs)

---

## üéØ Pr√≥ximos Pasos Inmediatos

**Para deployar HOY**:

1. ‚¨ú Comprimir datos: `python comprimir_boletines.py`
2. ‚¨ú Crear repo GitHub `sibom-data` (p√∫blico)
3. ‚¨ú Subir archivos .gz a GitHub
4. ‚¨ú Configurar `.env.local` con tu usuario GitHub
5. ‚¨ú Probar local: `npm run dev`
6. ‚¨ú Deploy a Vercel
7. ‚¨ú Configurar variables en Vercel
8. ‚¨ú Verificar funcionamiento

**Tiempo estimado**: ~30-45 minutos

---

## üÜò Si Necesitas Ayuda

1. **Revisa** `docs/DEPLOYMENT_GITHUB_VERCEL.md` (troubleshooting completo)
2. **Verifica logs** en Vercel ‚Üí Functions ‚Üí Logs
3. **Prueba acceso** a GitHub Raw manualmente
4. **Verifica variables** en Vercel Settings

---

## ‚ú® Resultado Final

Tendr√°s un chatbot:
- ‚úÖ Desplegado en Vercel (gratis)
- ‚úÖ Datos en GitHub (gratis)
- ‚úÖ LLM econ√≥mico ($0.15/mes con Gemini)
- ‚úÖ Cache optimizado (respuestas r√°pidas)
- ‚úÖ 80% ahorro bandwidth (gzip)
- ‚úÖ Escalable hasta 100 GB/mes

**Costo total mensual**: ~$0.15 USD

---

¬°√âxito con el deployment! üöÄ
