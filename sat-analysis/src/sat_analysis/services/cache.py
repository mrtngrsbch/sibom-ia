"""Sistema de cach√© para im√°genes satelitales.

Implementa FEAT-001: Cach√© de im√°genes descargadas para evitar
descargas repetidas de las mismas im√°genes Sentinel-2.
"""
from __future__ import annotations

import hashlib
import json
import logging
import pickle
from datetime import datetime
from pathlib import Path
from typing import Any

import numpy as np


logger = logging.getLogger(__name__)


class ImageCache:
    """Cach√© para im√°genes satelitales descargadas.

    Almacena bandas Sentinel-2 en disco para evitar descargarlas
    repetidamente. Cada entrada de cach√© incluye:
    - Los datos de las bandas (arrays numpy)
    - Metadatos (CRS, transform, fecha)
    - Timestamp de creaci√≥n

    El cach√© usa una clave basada en:
    - ID del item STAC
    - BBOX de descarga
    - Hash de los par√°metros
    """

    def __init__(
        self,
        cache_dir: str | Path | None = None,
        max_size_mb: float = 5000.0,
        ttl_days: int = 30,
    ):
        """Inicializa el cach√©.

        Args:
            cache_dir: Directorio donde almacenar el cach√© (default: logs/cache/)
            max_size_mb: Tama√±o m√°ximo del cach√© en MB
            ttl_days: Tiempo de vida en d√≠as para las entradas
        """
        if cache_dir is None:
            # Default: logs/cache/ dentro del proyecto sat-analysis
            cache_dir = Path(__file__).parent.parent.parent.parent / "logs" / "cache"

        self.cache_dir = Path(cache_dir)
        self.max_size_mb = max_size_mb
        self.ttl_days = ttl_days

        # Crear directorio si no existe
        self.cache_dir.mkdir(parents=True, exist_ok=True)

        # Archivo de √≠ndice
        self.index_file = self.cache_dir / "index.json"
        self.index = self._load_index()

    def _load_index(self) -> dict[str, Any]:
        """Carga el √≠ndice del cach√© desde disco."""
        if self.index_file.exists():
            try:
                with open(self.index_file, "r") as f:
                    return json.load(f)
            except Exception as e:
                logger.warning(f"Error cargando √≠ndice de cach√©: {e}")
        return {}

    def _save_index(self):
        """Guarda el √≠ndice del cach√© a disco."""
        try:
            with open(self.index_file, "w") as f:
                json.dump(self.index, f, indent=2)
        except Exception as e:
            logger.error(f"Error guardando √≠ndice de cach√©: {e}")

    def _generate_key(
        self,
        item_id: str,
        bbox: list[float] | None,
        geometry: dict | None,
    ) -> str:
        """Genera una clave √∫nica para la entrada de cach√©.

        Args:
            item_id: ID del item STAC
            bbox: BBOX de descarga
            geometry: Geometr√≠a de la parcela (opcional)

        Returns:
            Clave hash hexadecimal
        """
        # Crear string base para el hash
        key_parts = [item_id]

        if bbox is not None:
            key_parts.append(f"bbox:{bbox}")

        if geometry is not None:
            # Hash simple de la geometr√≠a
            geom_str = json.dumps(geometry, sort_keys=True)
            geom_hash = hashlib.md5(geom_str.encode()).hexdigest()[:8]
            key_parts.append(f"geom:{geom_hash}")

        key_string = "|".join(key_parts)
        return hashlib.sha256(key_string.encode()).hexdigest()

    def _get_cache_path(self, key: str) -> Path:
        """Retorna la ruta del archivo para una clave dada."""
        return self.cache_dir / f"{key}.npz"

    def _is_expired(self, entry: dict[str, Any]) -> bool:
        """Verifica si una entrada de cach√© ha expirado."""
        if "created_at" not in entry:
            return True

        created_at = datetime.fromisoformat(entry["created_at"])
        age_days = (datetime.now() - created_at).days

        return age_days > self.ttl_days

    def get(
        self,
        item_id: str,
        bbox: list[float] | None = None,
        geometry: dict | None = None,
    ) -> dict[str, Any] | None:
        """Obtiene bandas desde el cach√© si existen.

        Args:
            item_id: ID del item STAC
            bbox: BBOX de descarga
            geometry: Geometr√≠a de la parcela (opcional)

        Returns:
            Diccionario con las bandas y metadatos, o None si no est√° en cach√©
        """
        key = self._generate_key(item_id, bbox, geometry)

        if key not in self.index:
            return None

        entry = self.index[key]

        # Verificar expiraci√≥n
        if self._is_expired(entry):
            logger.info(f"Cach√© expirado para key={key[:8]}...")
            self.delete(key)
            return None

        # Cargar datos desde disco
        cache_path = self._get_cache_path(key)
        if not cache_path.exists():
            logger.warning(f"Archivo de cach√© no encontrado: {cache_path}")
            del self.index[key]
            self._save_index()
            return None

        try:
            data = np.load(cache_path, allow_pickle=True)

            # Retornar diccionario con bandas y metadatos
            result = {
                "b02": data["b02"],
                "b03": data["b03"],
                "b04": data["b04"],
                "b08": data["b08"],
                "b11": data["b11"],
                "b12": data["b12"],
                "crs": str(data["crs"]),
                "transform": tuple(data["transform"]),
                "shape": tuple(data["shape"]),
                "from_cache": True,
                "cache_key": key,
            }

            logger.info(f"‚úÖ Cach√© HIT: {item_id} ({cache_path.stat().st_size / 1024 / 1024:.1f} MB)")
            return result

        except Exception as e:
            logger.error(f"Error cargando cach√©: {e}")
            return None

    def put(
        self,
        item_id: str,
        bands: dict[str, np.ndarray],
        crs: str,
        transform: tuple,
        bbox: list[float] | None = None,
        geometry: dict | None = None,
    ) -> bool:
        """Guarda bandas en el cach√©.

        Args:
            item_id: ID del item STAC
            bands: Diccionario con las bandas (b02, b03, b04, b08, b11, b12)
            crs: CRS de las bandas
            transform: Transform affine
            bbox: BBOX de descarga
            geometry: Geometr√≠a de la parcela (opcional)

        Returns:
            True si se guard√≥ correctamente, False en caso contrario
        """
        key = self._generate_key(item_id, bbox, geometry)
        cache_path = self._get_cache_path(key)

        try:
            # Guardar arrays numpy comprimidos
            np.savez_compressed(
                cache_path,
                b02=bands["b02"],
                b03=bands["b03"],
                b04=bands["b04"],
                b08=bands["b08"],
                b11=bands["b11"],
                b12=bands["b12"],
                crs=crs,
                transform=transform,
                shape=bands["b02"].shape,
            )

            # Actualizar √≠ndice
            file_size_mb = cache_path.stat().st_size / 1024 / 1024

            self.index[key] = {
                "item_id": item_id,
                "created_at": datetime.now().isoformat(),
                "file_size_mb": file_size_mb,
                "bbox": bbox,
            }

            self._save_index()

            # Limpiar cach√© si excede el tama√±o m√°ximo
            self._cleanup_if_needed()

            logger.info(f"‚úÖ Cach√© PUT: {item_id} ({file_size_mb:.1f} MB)")
            return True

        except Exception as e:
            logger.error(f"Error guardando en cach√©: {e}")
            return False

    def delete(self, key: str) -> bool:
        """Elimina una entrada del cach√©.

        Args:
            key: Clave de la entrada a eliminar

        Returns:
            True si se elimin√≥ correctamente
        """
        cache_path = self._get_cache_path(key)

        try:
            if cache_path.exists():
                cache_path.unlink()

            if key in self.index:
                del self.index[key]
                self._save_index()

            return True

        except Exception as e:
            logger.error(f"Error eliminando del cach√©: {e}")
            return False

    def _cleanup_if_needed(self):
        """Limpia entradas antiguas si el cach√© excede el tama√±o m√°ximo."""
        total_size = sum(e.get("file_size_mb", 0) for e in self.index.values())

        if total_size <= self.max_size_mb:
            return

        logger.info(f"Cach√© excede tama√±o m√°ximo ({total_size:.1f} MB > {self.max_size_mb} MB), limpiando...")

        # Ordenar por fecha de creaci√≥n (m√°s antiguas primero)
        entries = sorted(
            self.index.items(),
            key=lambda x: x[1].get("created_at", ""),
        )

        # Eliminar entradas hasta estar por debajo del l√≠mite
        for key, entry in entries:
            if total_size <= self.max_size_mb * 0.8:  # Dejar margen del 20%
                break

            size_mb = entry.get("file_size_mb", 0)
            if self.delete(key):
                total_size -= size_mb
                logger.info(f"  Eliminada entrada antigua: {key[:8]}... ({size_mb:.1f} MB)")

    def clear(self):
        """Limpia todo el cach√©."""
        for key in list(self.index.keys()):
            self.delete(key)

        logger.info("üóëÔ∏è Cach√© limpiado completamente")

    def stats(self) -> dict[str, Any]:
        """Retorna estad√≠sticas del cach√©.

        Returns:
            Diccionario con estad√≠sticas
        """
        total_size = sum(e.get("file_size_mb", 0) for e in self.index.values())
        entry_count = len(self.index)

        return {
            "entry_count": entry_count,
            "total_size_mb": total_size,
            "max_size_mb": self.max_size_mb,
            "usage_percentage": (total_size / self.max_size_mb * 100) if self.max_size_mb > 0 else 0,
            "cache_dir": str(self.cache_dir),
        }
